{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CGHGHkqE4rJw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "import tqdm.notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "from pyspark.sql import SparkSession, Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3duPlU1fjxPz"
   },
   "source": [
    "## Initialize Spark Context and Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jovyan\n",
      " * Starting OpenBSD Secure Shell server sshd\n",
      "start-stop-daemon: unable to set gid to 0 (Operation not permitted)\n",
      "   ...fail!\n",
      " * sshd is running\n",
      "Starting namenodes on [localhost]\n",
      "localhost: Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\n",
      "localhost: namenode is running as process 158.  Stop it first and ensure /tmp/hadoop-jovyan-namenode.pid file is empty before retry.\n",
      "Starting datanodes\n",
      "localhost: Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\n",
      "localhost: datanode is running as process 263.  Stop it first and ensure /tmp/hadoop-jovyan-datanode.pid file is empty before retry.\n",
      "Starting secondary namenodes [470b66c3c538]\n",
      "470b66c3c538: Warning: Permanently added '470b66c3c538' (ED25519) to the list of known hosts.\n",
      "470b66c3c538: secondarynamenode is running as process 431.  Stop it first and ensure /tmp/hadoop-jovyan-secondarynamenode.pid file is empty before retry.\n",
      "Starting resourcemanager\n",
      "resourcemanager is running as process 721.  Stop it first and ensure /tmp/hadoop-jovyan-resourcemanager.pid file is empty before retry.\n",
      "Starting nodemanagers\n",
      "localhost: Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\n",
      "localhost: nodemanager is running as process 826.  Stop it first and ensure /tmp/hadoop-jovyan-nodemanager.pid file is empty before retry.\n",
      "WARNING: Use of this script to start the MR JobHistory daemon is deprecated.\n",
      "WARNING: Attempting to execute replacement \"mapred --daemon start\" instead.\n",
      "721 org.apache.hadoop.yarn.server.resourcemanager.ResourceManager\n",
      "16082 org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer\n",
      "263 org.apache.hadoop.hdfs.server.datanode.DataNode\n",
      "16106 sun.tools.jps.Jps -lm\n",
      "826 org.apache.hadoop.yarn.server.nodemanager.NodeManager\n",
      "158 org.apache.hadoop.hdfs.server.namenode.NameNode\n",
      "15278 org.apache.spark.deploy.SparkSubmit --conf spark.master=yarn --conf spark.executor.heartbeatInterval=60s --conf spark.app.name=spark_hw pyspark-shell\n",
      "431 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode\n",
      "Configured Capacity: 125130375168 (116.54 GB)\n",
      "Present Capacity: 3910414336 (3.64 GB)\n",
      "DFS Remaining: 1116073984 (1.04 GB)\n",
      "DFS Used: 2794340352 (2.60 GB)\n",
      "DFS Used%: 71.46%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (1):\n",
      "\n",
      "Name: 127.0.0.1:9866 (localhost)\n",
      "Hostname: 470b66c3c538\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 125130375168 (116.54 GB)\n",
      "DFS Used: 2794340352 (2.60 GB)\n",
      "Non DFS Used: 114816413696 (106.93 GB)\n",
      "DFS Remaining: 1116073984 (1.04 GB)\n",
      "DFS Used%: 2.23%\n",
      "DFS Remaining%: 0.89%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Sun May 07 11:22:36 UTC 2023\n",
      "Last Block Report: Sun May 07 09:39:45 UTC 2023\n",
      "Num of Blocks: 209\n",
      "\n",
      "\n",
      "chmod: changing permissions of '/home/jovyan/jupyter.log': Operation not permitted\n",
      "chmod: changing permissions of '/home/jovyan/nginx.log': Operation not permitted\n",
      "chmod: changing permissions of '/home/jovyan/error.log': Operation not permitted\n",
      "chmod: changing permissions of '/home/jovyan/access.log': Operation not permitted\n"
     ]
    }
   ],
   "source": [
    "! /home/jovyan/start-hadoop.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2023-05-07 11:25:01,512 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# I used this piece of code because simple ps.SparkContext didn't work on my machine :(\n",
    "\n",
    "conf = ps.SparkConf().setMaster(\"yarn\").setAppName(\"spark_advanced_hw\")\n",
    "conf.set(\"spark.executor.heartbeatInterval\", \"60s\")\n",
    "sc = ps.SparkContext('local[1]', '', conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "se = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1NJypwklF_Z"
   },
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! mkdir -p ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/jovyan/.kaggle/kaggle.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/.kaggle/kaggle.json\n",
    "{\"username\":\"pavelsyomin\",\"key\":\"78cb130c1d8b31a4180af8db35ee3f89\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "j4FsSCqe5CvG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (1.24.3)\n",
      "Collecting urllib3\n",
      "  Using cached urllib3-2.0.2-py3-none-any.whl (123 kB)\n",
      "Requirement already satisfied: kaggle==1.5.3 in /opt/conda/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kaggle==1.5.3) (2.28.2)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kaggle==1.5.3) (2.8.2)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kaggle==1.5.3) (2022.12.7)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kaggle==1.5.3) (4.65.0)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kaggle==1.5.3) (1.16.0)\n",
      "Requirement already satisfied: python-slugify in /opt/conda/lib/python3.10/site-packages (from kaggle==1.5.3) (8.0.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.10/site-packages (from python-slugify->kaggle==1.5.3) (1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle==1.5.3) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle==1.5.3) (2.1.1)\n",
      "Downloading page_views_sample.csv.zip to /home/jovyan/work\n",
      "100%|███████████████████████████████████████▊| 148M/149M [00:33<00:00, 4.55MB/s]\n",
      "100%|████████████████████████████████████████| 149M/149M [00:33<00:00, 4.64MB/s]\n",
      "Downloading documents_topics.csv.zip to /home/jovyan/work\n",
      "100%|████████████████████████████████████████| 121M/121M [00:27<00:00, 4.56MB/s]\n",
      "100%|████████████████████████████████████████| 121M/121M [00:27<00:00, 4.63MB/s]\n"
     ]
    }
   ],
   "source": [
    "# We need only page_views_sample and document_topics\n",
    "! pip install -U urllib3 kaggle==1.5.3\n",
    "! kaggle competitions download -c outbrain-click-prediction -f page_views_sample.csv.zip\n",
    "! kaggle competitions download -c outbrain-click-prediction -f documents_topics.csv.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  documents_topics.csv.zip\n",
      "  inflating: documents_topics.csv    \n",
      "\n",
      "Archive:  page_views_sample.csv.zip\n",
      "  inflating: page_views_sample.csv   \n",
      "\n",
      "2 archives were successfully processed.\n"
     ]
    }
   ],
   "source": [
    "! unzip '*.zip'\n",
    "! rm -rf *.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -put page_views_sample.csv\n",
    "! hdfs dfs -put documents_topics.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwxr-xr-x   - jovyan supergroup          0 2023-05-07 05:54 .sparkStaging\n",
      "-rw-r--r--   1 jovyan supergroup  339473038 2023-05-07 11:31 documents_topics.csv\n",
      "-rw-r--r--   1 jovyan supergroup  454346554 2023-05-07 11:31 page_views_sample.csv\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data to Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "V5Qx5EkolI_0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|          uuid|document_id|timestamp|platform|geo_location|traffic_source|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|1fd5f051fba643|        120| 31905835|       1|          RS|             2|\n",
      "|8557aa9004be3b|        120| 32053104|       1|       VN>44|             2|\n",
      "|c351b277a358f0|        120| 54013023|       1|       KR>12|             1|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "page_views = se.read.csv(\"page_views_sample.csv\", header=True)\n",
    "page_views.registerTempTable(\"page_views\")\n",
    "page_views.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------------------+\n",
      "|document_id|topic_id|  confidence_level|\n",
      "+-----------+--------+------------------+\n",
      "|    1595802|     140|0.0731131601068925|\n",
      "|    1595802|      16|0.0594164867373976|\n",
      "|    1595802|     143|0.0454207537554526|\n",
      "+-----------+--------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents_topics = se.read.csv(\"documents_topics.csv\", header=True)\n",
    "documents_topics.registerTempTable(\"document_topics\")\n",
    "documents_topics.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hK93Ci6Rj51c"
   },
   "source": [
    "## Complete the tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Jf8VBI-j530"
   },
   "source": [
    "Data: outbrain click prediction\n",
    "\n",
    "Tasks:\n",
    "Using Spark RDD, DataFrame API and Python, calculate:\n",
    "\n",
    "**1**. Top 10 most visited document_ids in the page_views_sample log\n",
    "\n",
    "**2**. How many users have at least 2 different traffic_sources in the page_views_sample log (note the value is not a count, it's an encoded enum)\n",
    "\n",
    "**3***. Top 10 most visited topic_ids in page_views_sample log (use documents_topics table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submission format is the result.json json file with top_10_documents, users and top_10_topics keys.\n",
    "For TOP-10 results, the answer must be written in the form of a sheet ordered from TOP-1 to TOP-10 with an id.\n",
    "\n",
    "result.json example:\n",
    "\n",
    "    {\n",
    "        \"top_10_documents\": [\n",
    "            111,\n",
    "            222,\n",
    "            333,\n",
    "            ...,\n",
    "            1010\n",
    "        ],\n",
    "        \"users\": 10000,\n",
    "        \"top_10_topics\": [\n",
    "            11,\n",
    "            22,\n",
    "            33,\n",
    "            ...,\n",
    "            101\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: top-10 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1811567,\n",
       " 234,\n",
       " 42744,\n",
       " 1858440,\n",
       " 1780813,\n",
       " 60164,\n",
       " 1790442,\n",
       " 1877626,\n",
       " 1821895,\n",
       " 732651]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_documents = [\n",
    "    int(row.document_id) \n",
    "    for row in page_views.groupby(\"document_id\").count().sort(\"count\", ascending=False).head(10)\n",
    "]\n",
    "top_10_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: users with more than one traffic source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98080"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = se.sql(\"\"\"\n",
    "    SELECT uuid, count(DISTINCT traffic_source) as cnt\n",
    "    FROM page_views\n",
    "    GROUP BY uuid\n",
    "    HAVING count(DISTINCT traffic_source) > 1\n",
    "\"\"\").count()\n",
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: top-10 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[20, 16, 216, 136, 140, 143, 36, 97, 8, 269]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_topics = [\n",
    "    int(row.topic_id)\n",
    "    for row in (\n",
    "        page_views\n",
    "          .join(documents_topics, page_views.document_id == documents_topics.document_id, \"left\")\n",
    "          .select(page_views.document_id, documents_topics.topic_id)\n",
    "          .groupby(\"topic_id\")\n",
    "          .count()\n",
    "          .sort(\"count\", ascending=False)\n",
    "          .limit(10)\n",
    "          .collect()\n",
    "    )\n",
    "]\n",
    "top_10_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare result and submit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'top_10_documents': [1811567,\n",
       "  234,\n",
       "  42744,\n",
       "  1858440,\n",
       "  1780813,\n",
       "  60164,\n",
       "  1790442,\n",
       "  1877626,\n",
       "  1821895,\n",
       "  732651],\n",
       " 'users': 98080,\n",
       " 'top_10_topics': [20, 16, 216, 136, 140, 143, 36, 97, 8, 269]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "result = {\n",
    "    \"top_10_documents\": top_10_documents,\n",
    "    \"users\": users,\n",
    "    \"top_10_topics\": top_10_topics\n",
    "}\n",
    "with open(\"result.json\", \"w\") as f:\n",
    "    json.dump(result, f)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "! curl -F file=@result.json \"51.250.54.133:80/MDS-LSML1/findabalance22/w4/1\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "spark_seminar (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
